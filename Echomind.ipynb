{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "53KbmjoWJ-T6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuZzrjG7KBQn",
        "outputId": "5f927963-90e7-4010-947a-09f0fccdab3b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt_tab\")\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "# Import necessary libraries\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import pickle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnJ7fR-9KIaf",
        "outputId": "7c7f8f57-4a70-4565-b29e-e15641938512"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout"
      ],
      "metadata": {
        "id": "lvfEtKOhKKdF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"intents.json\") as json_data:\n",
        "    intents = json.load(json_data)"
      ],
      "metadata": {
        "id": "06eyeBgyKORC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t3lsFbhnKPyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Empty lists for appending the data after processing NLP\n",
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore = [\"?\"]\n",
        "\n",
        "# Tokenize and process each pattern\n",
        "for intent in intents[\"intents\"]:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        words.extend(w)\n",
        "        documents.append((w, intent[\"tag\"]))\n",
        "        if intent[\"tag\"] not in classes:\n",
        "            classes.append(intent[\"tag\"])"
      ],
      "metadata": {
        "id": "ytc94M2zDEX0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Performing Stemming by using stemmer.stem() nd lower each word\n",
        "#Running loop in words[] and ignoring punctuation marks present in ignore[]\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore]\n",
        "\n",
        "#Removing Duplicate\n",
        "words = sorted(list(set(words)))\n",
        "classes = sorted(list(set(classes)))"
      ],
      "metadata": {
        "id": "rxQPfLr3KRhG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(documents), \"documents\")\n",
        "print(len(classes), \"classes\")\n",
        "print(len(words), \"unique stemmed words\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZlYM-EaKTR-",
        "outputId": "a337593e-ed4f-412b-bcb7-6160eb3e35de"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "106 documents\n",
            "37 classes\n",
            "118 unique stemmed words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Training Data which will be furthur used for training\n",
        "training = []\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "for doc in documents:\n",
        "    bag = []\n",
        "    pattern_words = doc[0] #Storing list of tokenized words for the documents[] tp pattern_words\n",
        "\n",
        "       #Again Stemming each word from pattern_words\n",
        "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "\n",
        "      #Creating bag of words array\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    #It will give output 1 for curent tag and 0 for all other tags\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "    training.append([bag, output_row])\n"
      ],
      "metadata": {
        "id": "MLs1mMyFKUtC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VS3_zZt9KXHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(training) #Suffling the data or features\n",
        "\n",
        "# Separate the data into train_x and train_y\n",
        "train_x = []\n",
        "train_y = []\n",
        "\n",
        "for feature, label in training:\n",
        "    train_x.append(feature)  # Bag of words (features)\n",
        "    train_y.append(label)    # Output row (labels)\n",
        "\n",
        "# Convert to numpy arrays\n",
        "train_x = np.array(train_x)\n",
        "train_y = np.array(train_y)\n"
      ],
      "metadata": {
        "id": "cQe3zPQTDNUy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the Keras model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h9r6c2hKYnu",
        "outputId": "6f5978aa-00e3-4dc9-dae5-c1663b1e7d21"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "AQKVvgs8KaU5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.config.run_functions_eagerly(True)\n",
        "print(\"Eager execution enabled:\", tf.executing_eagerly())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYOimzKxKeV8",
        "outputId": "d7f69109-bed0-41d9-eb77-f5cc6ccb85de"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eager execution enabled: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Ensure eager execution is enabled (this is default in TensorFlow 2.x)\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "# Define a simple neural network with Keras layers\n",
        "input_dim = len(train_x[0])\n",
        "output_dim = len(train_y[0])\n",
        "\n",
        "# Creating the model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.InputLayer(input_shape=(input_dim,)),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(output_dim, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Convert training data to numpy arrays\n",
        "train_x = np.array(train_x)\n",
        "train_y = np.array(train_y)\n",
        "\n",
        "# Define the custom training step using tf.function\n",
        "@tf.function\n",
        "def train_step(x_batch, y_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Forward pass\n",
        "        predictions = model(x_batch, training=True)\n",
        "        loss = tf.keras.losses.categorical_crossentropy(y_batch, predictions)\n",
        "\n",
        "    # Compute gradients\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "    # Apply gradients (updating the weights)\n",
        "    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return loss\n",
        "\n",
        "# Training loop without using tf.data.Dataset\n",
        "def custom_training(train_x, train_y, epochs, batch_size):\n",
        "    num_samples = train_x.shape[0]\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        for step in range(0, num_samples, batch_size):\n",
        "            x_batch = train_x[step:step + batch_size]\n",
        "            y_batch = train_y[step:step + batch_size]\n",
        "            loss = train_step(x_batch, y_batch)\n",
        "            if step % 100 == 0:\n",
        "                print(f\"Step {step}: Loss = {tf.reduce_mean(loss)}\")\n",
        "\n",
        "# Train the model using the custom training loop\n",
        "custom_training(train_x, train_y, epochs=100, batch_size=8)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srkSchC2KgKk",
        "outputId": "71331055-47ce-425b-d117-104e74227828"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "Step 0: Loss = 3.6318752765655518\n",
            "Epoch 2/100\n",
            "Step 0: Loss = 3.6033129692077637\n",
            "Epoch 3/100\n",
            "Step 0: Loss = 3.5906548500061035\n",
            "Epoch 4/100\n",
            "Step 0: Loss = 3.5799622535705566\n",
            "Epoch 5/100\n",
            "Step 0: Loss = 3.569514513015747\n",
            "Epoch 6/100\n",
            "Step 0: Loss = 3.5589680671691895\n",
            "Epoch 7/100\n",
            "Step 0: Loss = 3.546220064163208\n",
            "Epoch 8/100\n",
            "Step 0: Loss = 3.5304908752441406\n",
            "Epoch 9/100\n",
            "Step 0: Loss = 3.5119881629943848\n",
            "Epoch 10/100\n",
            "Step 0: Loss = 3.4933266639709473\n",
            "Epoch 11/100\n",
            "Step 0: Loss = 3.472608804702759\n",
            "Epoch 12/100\n",
            "Step 0: Loss = 3.4503376483917236\n",
            "Epoch 13/100\n",
            "Step 0: Loss = 3.4260785579681396\n",
            "Epoch 14/100\n",
            "Step 0: Loss = 3.397939682006836\n",
            "Epoch 15/100\n",
            "Step 0: Loss = 3.3645267486572266\n",
            "Epoch 16/100\n",
            "Step 0: Loss = 3.326995372772217\n",
            "Epoch 17/100\n",
            "Step 0: Loss = 3.2823026180267334\n",
            "Epoch 18/100\n",
            "Step 0: Loss = 3.233091115951538\n",
            "Epoch 19/100\n",
            "Step 0: Loss = 3.1797895431518555\n",
            "Epoch 20/100\n",
            "Step 0: Loss = 3.1241707801818848\n",
            "Epoch 21/100\n",
            "Step 0: Loss = 3.065528392791748\n",
            "Epoch 22/100\n",
            "Step 0: Loss = 3.0052592754364014\n",
            "Epoch 23/100\n",
            "Step 0: Loss = 2.9394874572753906\n",
            "Epoch 24/100\n",
            "Step 0: Loss = 2.8725147247314453\n",
            "Epoch 25/100\n",
            "Step 0: Loss = 2.8041415214538574\n",
            "Epoch 26/100\n",
            "Step 0: Loss = 2.7351088523864746\n",
            "Epoch 27/100\n",
            "Step 0: Loss = 2.6672630310058594\n",
            "Epoch 28/100\n",
            "Step 0: Loss = 2.5997142791748047\n",
            "Epoch 29/100\n",
            "Step 0: Loss = 2.5339488983154297\n",
            "Epoch 30/100\n",
            "Step 0: Loss = 2.469249725341797\n",
            "Epoch 31/100\n",
            "Step 0: Loss = 2.4030299186706543\n",
            "Epoch 32/100\n",
            "Step 0: Loss = 2.3368723392486572\n",
            "Epoch 33/100\n",
            "Step 0: Loss = 2.2713818550109863\n",
            "Epoch 34/100\n",
            "Step 0: Loss = 2.2087178230285645\n",
            "Epoch 35/100\n",
            "Step 0: Loss = 2.145822525024414\n",
            "Epoch 36/100\n",
            "Step 0: Loss = 2.0857198238372803\n",
            "Epoch 37/100\n",
            "Step 0: Loss = 2.0254721641540527\n",
            "Epoch 38/100\n",
            "Step 0: Loss = 1.9687416553497314\n",
            "Epoch 39/100\n",
            "Step 0: Loss = 1.913525104522705\n",
            "Epoch 40/100\n",
            "Step 0: Loss = 1.8589969873428345\n",
            "Epoch 41/100\n",
            "Step 0: Loss = 1.8062779903411865\n",
            "Epoch 42/100\n",
            "Step 0: Loss = 1.7542164325714111\n",
            "Epoch 43/100\n",
            "Step 0: Loss = 1.7028617858886719\n",
            "Epoch 44/100\n",
            "Step 0: Loss = 1.6555805206298828\n",
            "Epoch 45/100\n",
            "Step 0: Loss = 1.6086199283599854\n",
            "Epoch 46/100\n",
            "Step 0: Loss = 1.561792016029358\n",
            "Epoch 47/100\n",
            "Step 0: Loss = 1.51918625831604\n",
            "Epoch 48/100\n",
            "Step 0: Loss = 1.4759318828582764\n",
            "Epoch 49/100\n",
            "Step 0: Loss = 1.434700608253479\n",
            "Epoch 50/100\n",
            "Step 0: Loss = 1.3934299945831299\n",
            "Epoch 51/100\n",
            "Step 0: Loss = 1.3543474674224854\n",
            "Epoch 52/100\n",
            "Step 0: Loss = 1.3132333755493164\n",
            "Epoch 53/100\n",
            "Step 0: Loss = 1.2771625518798828\n",
            "Epoch 54/100\n",
            "Step 0: Loss = 1.2380143404006958\n",
            "Epoch 55/100\n",
            "Step 0: Loss = 1.2005680799484253\n",
            "Epoch 56/100\n",
            "Step 0: Loss = 1.165038824081421\n",
            "Epoch 57/100\n",
            "Step 0: Loss = 1.1290556192398071\n",
            "Epoch 58/100\n",
            "Step 0: Loss = 1.0933618545532227\n",
            "Epoch 59/100\n",
            "Step 0: Loss = 1.0596027374267578\n",
            "Epoch 60/100\n",
            "Step 0: Loss = 1.0246118307113647\n",
            "Epoch 61/100\n",
            "Step 0: Loss = 0.9906579256057739\n",
            "Epoch 62/100\n",
            "Step 0: Loss = 0.9584457874298096\n",
            "Epoch 63/100\n",
            "Step 0: Loss = 0.9238211512565613\n",
            "Epoch 64/100\n",
            "Step 0: Loss = 0.8926621675491333\n",
            "Epoch 65/100\n",
            "Step 0: Loss = 0.861871600151062\n",
            "Epoch 66/100\n",
            "Step 0: Loss = 0.8295592069625854\n",
            "Epoch 67/100\n",
            "Step 0: Loss = 0.7986222505569458\n",
            "Epoch 68/100\n",
            "Step 0: Loss = 0.7705765962600708\n",
            "Epoch 69/100\n",
            "Step 0: Loss = 0.7397029399871826\n",
            "Epoch 70/100\n",
            "Step 0: Loss = 0.7117488384246826\n",
            "Epoch 71/100\n",
            "Step 0: Loss = 0.6845885515213013\n",
            "Epoch 72/100\n",
            "Step 0: Loss = 0.6574442982673645\n",
            "Epoch 73/100\n",
            "Step 0: Loss = 0.6323287487030029\n",
            "Epoch 74/100\n",
            "Step 0: Loss = 0.6072478890419006\n",
            "Epoch 75/100\n",
            "Step 0: Loss = 0.5832377076148987\n",
            "Epoch 76/100\n",
            "Step 0: Loss = 0.5603023171424866\n",
            "Epoch 77/100\n",
            "Step 0: Loss = 0.5372159481048584\n",
            "Epoch 78/100\n",
            "Step 0: Loss = 0.515254557132721\n",
            "Epoch 79/100\n",
            "Step 0: Loss = 0.495332270860672\n",
            "Epoch 80/100\n",
            "Step 0: Loss = 0.47523802518844604\n",
            "Epoch 81/100\n",
            "Step 0: Loss = 0.4569312334060669\n",
            "Epoch 82/100\n",
            "Step 0: Loss = 0.4379999339580536\n",
            "Epoch 83/100\n",
            "Step 0: Loss = 0.4208114743232727\n",
            "Epoch 84/100\n",
            "Step 0: Loss = 0.4046862721443176\n",
            "Epoch 85/100\n",
            "Step 0: Loss = 0.3882960081100464\n",
            "Epoch 86/100\n",
            "Step 0: Loss = 0.3724293112754822\n",
            "Epoch 87/100\n",
            "Step 0: Loss = 0.3577542304992676\n",
            "Epoch 88/100\n",
            "Step 0: Loss = 0.3437316119670868\n",
            "Epoch 89/100\n",
            "Step 0: Loss = 0.33011847734451294\n",
            "Epoch 90/100\n",
            "Step 0: Loss = 0.316398948431015\n",
            "Epoch 91/100\n",
            "Step 0: Loss = 0.3040468394756317\n",
            "Epoch 92/100\n",
            "Step 0: Loss = 0.2916386127471924\n",
            "Epoch 93/100\n",
            "Step 0: Loss = 0.2799351215362549\n",
            "Epoch 94/100\n",
            "Step 0: Loss = 0.26873573660850525\n",
            "Epoch 95/100\n",
            "Step 0: Loss = 0.2579341530799866\n",
            "Epoch 96/100\n",
            "Step 0: Loss = 0.24717973172664642\n",
            "Epoch 97/100\n",
            "Step 0: Loss = 0.2370588779449463\n",
            "Epoch 98/100\n",
            "Step 0: Loss = 0.22710266709327698\n",
            "Epoch 99/100\n",
            "Step 0: Loss = 0.21758973598480225\n",
            "Epoch 100/100\n",
            "Step 0: Loss = 0.20877519249916077\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.run_functions_eagerly(True)\n",
        "import keras.saving\n",
        "keras.saving.save_model(model, \"C:/Users/Shruti Jain/OneDrive/Desktop/proj/proj.h5\")\n",
        "pickle.dump({\"words\": words, \"classes\": classes, \"train_x\": train_x, \"train_y\": train_y}, open(\"training_data\", \"wb\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gT3g3UhDKiPh",
        "outputId": "94340f89-d73c-4eb4-85fa-3daeab3c6600"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import pickle\n",
        "\n",
        "model = load_model(\"C:/Users/Shruti Jain/OneDrive/Desktop/proj/proj.h5\")\n",
        "\n",
        "# Recompile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Loading training data\n",
        "data = pickle.load(open(\"training_data\", \"rb\"))\n",
        "words = data['words']\n",
        "classes = data['classes']\n",
        "train_x = data['train_x']\n",
        "train_y = data['train_y']\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K49xqLKFKxYc",
        "outputId": "a14028c8-9aa3-4771-972e-08af9654e88f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to clean up and tokenize user input\n",
        "def clean_up_sentence(sentence):\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n"
      ],
      "metadata": {
        "id": "j5KGn91MKz8T"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bag of words function\n",
        "def bow(sentence, words, show_details=False):\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    bag = [0]*len(words)\n",
        "    for s in sentence_words:\n",
        "        for i, w in enumerate(words):\n",
        "            if w == s:\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print(\"Found in bag: %s\" % w)\n",
        "    return np.array(bag)\n"
      ],
      "metadata": {
        "id": "6jxK7xm1K1zF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classify function to predict intent\n",
        "ERROR_THRESHOLD = 0.25\n",
        "def classify(sentence):\n",
        "    bow_data = bow(sentence, words)\n",
        "    results = model.predict(np.array([bow_data]))[0]\n",
        "    results = [[i, r] for i, r in enumerate(results) if r > ERROR_THRESHOLD]\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = [(classes[r[0]], r[1]) for r in results]\n",
        "    return return_list\n"
      ],
      "metadata": {
        "id": "4sqP176jK3aq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Response function to generate response\n",
        "context = {}\n",
        "def response(sentence, userID='123', show_details=False):\n",
        "    results = classify(sentence)\n",
        "    if results:\n",
        "        while results:\n",
        "            for i in intents['intents']:\n",
        "                if i['tag'] == results[0][0]:\n",
        "                    if 'context_set' in i:\n",
        "                        context[userID] = i['context_set']\n",
        "                    if not 'context_filter' in i or (userID in context and 'context_filter' in i and i['context_filter'] == context[userID]):\n",
        "                        return random.choice(i['responses'])\n",
        "            results.pop(0)"
      ],
      "metadata": {
        "id": "p1TBxptoK5L8"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response(\"Hello\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "SnyR_c_HK63Z",
        "outputId": "ada78d18-9a3d-4156-c84b-c4f0489dded2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello, thanks for visiting'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ATX6qYVK8x7",
        "outputId": "4aea7ccd-862b-4565-838a-17b0b8463e2b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.5.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.5-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting gradio-client==1.4.2 (from gradio)\n",
            "  Downloading gradio_client-1.4.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.11)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (11.0.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart==0.0.12 (from gradio)\n",
            "  Downloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.7.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<1.0,>=0.1.1 (from gradio)\n",
            "  Downloading safehttpx-0.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.13.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio) (2024.10.0)\n",
            "Collecting websockets<13.0,>=10.0 (from gradio-client==1.4.2->gradio)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.25.1->gradio) (4.66.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.25.1->gradio) (2.2.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.5.0-py3-none-any.whl (56.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.7/56.7 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.4.2-py3-none-any.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\n",
            "Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.5-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading ruff-0.7.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.1-py3-none-any.whl (8.4 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.41.2-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, websockets, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.5 ffmpy-0.4.0 gradio-5.5.0 gradio-client-1.4.2 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.12 ruff-0.7.3 safehttpx-0.1.1 semantic-version-2.10.0 starlette-0.41.2 tomlkit-0.12.0 uvicorn-0.32.0 websockets-12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "def chat_model(user_input, history):\n",
        "    # Get bot response\n",
        "    bot_response = response(user_input)\n",
        "    # Add to conversation history\n",
        "    history.append(f\"You: {user_input}\")\n",
        "    history.append(f\"Bot: {bot_response}\")\n",
        "\n",
        "    # Create a conversation string to display\n",
        "    conversation = \"\\n\".join(history)\n",
        "\n",
        "    return conversation, history  # Return the conversation and updated history\n",
        "\n",
        "\n",
        "# Define Gradio Interface\n",
        "interface = gr.Interface(\n",
        "    fn=chat_model,                # Function to call on input\n",
        "    inputs=[\"text\", \"state\"],     # Input type (text box for user input) and state for conversation history\n",
        "    outputs=[\"text\", \"state\"],    # Output type (conversation history and updated history state)\n",
        "    title=\"Chat with the Model\",  # Title of the chat UI\n",
        "    description=\"Type a message to chat with the model!\",  # Short description\n",
        "    live=False                    # Disable live mode, trigger only on Enter key\n",
        ")\n",
        "\n",
        "# Launch the Gradio interface\n",
        "interface.launch(share=True)  #"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "wwRSm5n8LAKS",
        "outputId": "7aa3689c-e12e-4c7d-9251-d5b550d7592e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a04e960f5fa5a25477.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a04e960f5fa5a25477.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ADxk2uW3LG-8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}